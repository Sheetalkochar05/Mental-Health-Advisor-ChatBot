# -*- coding: utf-8 -*-
"""VAC_BOTFORGE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I8pW4yp9hZTxD38nUg8XIIovTkZ2G6yv
"""

# === Setup ===
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate
from langchain.chains.question_answering import load_qa_chain
from langchain.document_loaders import GoogleDriveLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.memory import ConversationBufferMemory
import pinecone

# === Configure API Keys ===
GOOGLE_API_KEY = ""
GOOGLE_DRIVE_API_KEY = ""
PINECONE_API_KEY = ""
PINECONE_ENV = ""
PINECONE_INDEX_NAME = ""

# === Gemini Chat & Embeddings ===
llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)
embedding_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=GOOGLE_API_KEY)

# === Load & Embed CBT Document ===
loader = GoogleDriveLoader(document_ids=[""], credentials_path="credentials.json")
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
documents = text_splitter.split_documents(docs)

# === Initialize Pinecone Vector Store ===
pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)
vector_store = Pinecone.from_documents(documents, embedding_model, index_name=PINECONE_INDEX_NAME)

# === Memory ===
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# === Intake Agent ===
intake_questions = [
    "How would you describe your mood today on a scale of 1 to 10 (1 being very low, 10 being very high)?",
    "Have you been experiencing any significant changes in your sleep patterns recently (e.g., sleeping more or less than usual)?",
    "Have you noticed any changes in your appetite or eating habits lately?",
    "How would you rate your energy levels today? (Low, Medium, High)",
    "Can you describe any specific feelings or emotions you're currently experiencing?"
]

def run_intake_agent():
    responses = {}
    print("ðŸ‘©â€âš•ï¸ Intake Agent Initiated:")
    for i, question in enumerate(intake_questions, 1):
        answer = input(f"{i}. {question}\n> ")
        responses[f"Q{i}"] = answer
    return responses

# === Follow-Up Agent ===

followup_template = PromptTemplate.from_template(
    """You are a compassionate CBT therapist. Based on this user answer: "{previous_answer}",
ask a follow-up question to better understand their emotional or mental state.
Keep it supportive and thoughtful."""
)

stop_check_template = PromptTemplate.from_template(
    """As a therapist, you have asked the following follow-up questions and received these answers:

{history}

Based on this conversation so far, do you feel you understand the user's mental state well enough to proceed with CBT advice?

Respond with "YES" if you've gathered sufficient information, or "NO" if another follow-up question is still needed.
"""
)

def run_followup_agent(intake_data):
    print("\nðŸ”„ Follow-Up Agent:")
    followup_data = []

    for i, (key, initial_answer) in enumerate(intake_data.items(), 1):
        previous_answer = initial_answer
        history = ""

        while True:
            # Ask follow-up
            followup_prompt = followup_template.format(previous_answer=previous_answer)
            followup_question = llm.invoke(followup_prompt).content
            print(f"\nFollow-Up Q{i}: {followup_question}")
            user_reply = input("> ")

            # Store and build history
            followup_data.append((followup_question, user_reply))
            history += f"Q: {followup_question}\nA: {user_reply}\n\n"

            # Ask LLM if more follow-ups are needed
            stop_check_prompt = stop_check_template.format(history=history)
            decision = llm.invoke(stop_check_prompt).content.strip().lower()

            if "yes" in decision:
                break
            else:
                previous_answer = user_reply

    return followup_data

# === Advice Agent (RAG Chain) ===
qa_chain = load_qa_chain(llm=llm, chain_type="stuff")

retrieval_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vector_store.as_retriever(),
    memory=memory,
    combine_docs_chain=qa_chain
)

def run_advice_agent(intake_data, followup_data, user_concern):
    print("\nðŸ“˜ Advice Agent:")

    # Combine all context: intake, follow-up, and concern
    context = "Here are the user's responses:\n\n"

    for i, (q, a) in enumerate(intake_data.items(), 1):
        context += f"Intake Q{i}: {intake_questions[i-1]}\nAnswer: {a}\n\n"

    for i, (follow_q, follow_a) in enumerate(followup_data, 1):
        context += f"Follow-Up Q{i}: {follow_q}\nAnswer: {follow_a}\n\n"

    context += f"Main Concern: {user_concern}\n\n"
    context += "Please provide cognitive behavioral therapy (CBT) based advice that addresses the emotional and behavioral patterns reflected in the responses."

    # Run the RAG chain
    response = retrieval_chain.run(context)
    print("\nðŸ§  CBT Advice:", response)

# === Main Flow ===
if __name__ == "__main__":
    # Step 1: Intake
    intake_data = run_intake_agent()

    # Step 2: Follow-Up Loop (LLM controls when to stop)
    followup_data = run_followup_agent(intake_data)

    # Step 3: Final CBT Advice
    user_concern = input("\nðŸ“© Final Question: Please share a concern you'd like advice on.\n> ")
    run_advice_agent(intake_data, followup_data, user_concern)